# -*- coding: utf-8 -*-
"""dpo_train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jvTxAY0w6txvwxx9jiWEwM8jcTWp3WfP
"""
# !pip install bitsandbytes
# !pip install trl
# !pip install -U bitsandbytes

import torch
from peft import LoraConfig, get_peft_model
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from trl import DPOTrainer, DPOConfig
import sys
from data_util import load_data_ultra_feedback

from huggingface_hub import login
login(new_session=False)


logfile = open("train.log", "w")
sys.stdout = logfile
sys.stderr = logfile

"""# Get base model"""

device = torch.device('cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'))
print(device)

def get_model():
  model_name = 'google/gemma-2b-it'
  bnb_config = BitsAndBytesConfig(load_in_4bit=True,
                                  bnb_4bit_quant_type='nf4',
                                  bnb_4bit_compute_dtype=torch.float16)
  print(f'loading pretrained model on {device}')
  model = AutoModelForCausalLM.from_pretrained(model_name,
                                               quantization_config=bnb_config,
                                               device_map="auto")
  print('loading tokenizer')
  tokenizer = AutoTokenizer.from_pretrained(model_name)
  tokenizer.pad_token = tokenizer.eos_token

  # Setup LoRA
  peft_config = LoraConfig(
      r=16, # lower r to reduce memory
      lora_alpha=32,
      lora_dropout=0.05,
      target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'],
 #      target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'],
      bias='none',
      task_type='CAUSAL_LM')

  print('get peft model')
  # model.enable_input_require_grads()    
  model = get_peft_model(model, peft_config)
  model.gradient_checkpointing_enable()  # enable checkpointing to reduce memory cost
  model.config.use_cache = False  # disable KV cache to save memory
  print('model.get_input_embeddings().weight.requires_grad: ',model.get_input_embeddings().weight.requires_grad )
  model.print_trainable_parameters()

  return model,  tokenizer


"""# Prepare post training data"""

model,  tokenizer = get_model()
dataset = load_data_ultra_feedback()

"""# Train model"""

training_args = DPOConfig(
    output_dir="checkpoints/dpo-gemma2b_4",
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    learning_rate=2e-6,
    lr_scheduler_type="cosine",
    warmup_steps=500,
    logging_steps=100,
    save_steps=1000,
    max_steps=7000,
    beta=0.1,       # Important: strength of preference alignment
    max_length=512,
    max_prompt_length=256
)

trainer = DPOTrainer(
    model=model,
    ref_model=None,
    args=training_args,
    train_dataset=dataset
)

trainer.train()
logfile.close()
