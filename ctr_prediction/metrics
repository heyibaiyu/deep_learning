
MLP with embedding (1M data)
	○ Epoch 4, train_loss: 0.4356, val_loss: 0.4833, train_auc: 0.8169, val_auc: 0.7616, lr: 0.001000

Wide & Deep (google, 2016)
    ○ V1: Epoch 14, train_loss: 2.3588, val_loss: 4.7761, train_auc: 0.7416, val_auc: 0.6510 (so low), lr: 0.001000
        § Val auc is much lower than simple MLP. The reason is wide part use raw categorical id as feature (big int)
    ○ V2: Remove categorical feature from wide component
        § Epoch 3, train_loss: 0.4504, val_loss: 0.4772, train_auc: 0.8007, val_auc: 0.7659(recovered, but didn't improve too much), lr: 0.001000
    ○ V3: Use same feature as deep component in wide layer
        § Epoch 2, train_loss: 0.4644, val_loss: 0.4762, train_auc: 0.7836, val_auc: 0.7650 (wide doesn't learn anything from emb), lr: 0.001000
    ○ V4: V2 + bias=True in wide layer
        Epoch 3, train_loss: 0.4523, val_loss: 0.4741, train_auc: 0.7986, val_auc: 0.7670 (better), lr: 0.001000
    ○ V5-1 (10k data): one-hot-encoding categorical features and use them in wide layer together with numerical features.
        Epoch 8, train_loss: 0.3875, val_loss: 0.4846, train_auc: 0.8603, val_auc: 0.7229 (better than V1, but worse than V2), lr: 0.001000
      V5-2 (50k data):
        Epoch 3, train_loss: 0.4211, val_loss: 0.4688, train_auc: 0.8170, val_auc: 0.7422, lr: 0.001000

DeepFM (Huawei, 2017)
    ○ DeepFM shares the same raw feature input and latent feature embeddings between its FM and DNN components.
    ○ It eliminates the need for manual feature engineering
        Epoch 19, train_loss: 0.4951, val_loss: 0.6402, train_auc: 0.7742, val_auc: 0.7458, lr: 0.001000 (dataset: 1000000)

DCN (Google, 2017)
    ○ Epoch 9, train_loss: 0.4638, val_loss: 0.4624, train_auc: 0.7906, val_auc: 0.7814, lr: 0.001000




"""
data shape (2000000, 40)
Epoch 0, train_loss: 0.6618, val_loss: 0.4891, train_auc: 0.6599, val_auc: 0.7387
Epoch 1, train_loss: 0.4925, val_loss: 0.4763, train_auc: 0.7428, val_auc: 0.7570
Epoch 2, train_loss: 0.4808, val_loss: 0.4737, train_auc: 0.7636, val_auc: 0.7626
Epoch 3, train_loss: 0.4694, val_loss: 0.4755, train_auc: 0.7756, val_auc: 0.7588
Epoch 4, train_loss: 0.4535, val_loss: 0.4736, train_auc: 0.7935, val_auc: 0.7615
Epoch 5, train_loss: 0.4423, val_loss: 0.4796, train_auc: 0.8047, val_auc: 0.7561
Epoch 6, train_loss: 0.4280, val_loss: 0.4868, train_auc: 0.8198, val_auc: 0.7534
Epoch 7, train_loss: 0.4123, val_loss: 0.4952, train_auc: 0.8345, val_auc: 0.7496
Epoch 8, train_loss: 0.4036, val_loss: 0.5085, train_auc: 0.8434, val_auc: 0.7420
Epoch 9, train_loss: 0.3865, val_loss: 0.5180, train_auc: 0.8563, val_auc: 0.7428

data shape (1000000, 40)
Epoch 0, train_loss: 0.9024, val_loss: 0.5147, train_auc: 0.6274, val_auc: 0.7105
Epoch 1, train_loss: 0.5175, val_loss: 0.4929, train_auc: 0.7132, val_auc: 0.7437
Epoch 2, train_loss: 0.4918, val_loss: 0.4865, train_auc: 0.7494, val_auc: 0.7562
Epoch 3, train_loss: 0.4755, val_loss: 0.4871, train_auc: 0.7712, val_auc: 0.7539
Epoch 4, train_loss: 0.4687, val_loss: 0.4882, train_auc: 0.7818, val_auc: 0.7522
Epoch 5, train_loss: 0.4738, val_loss: 0.5005, train_auc: 0.7840, val_auc: 0.7424
Epoch 6, train_loss: 0.4426, val_loss: 0.4933, train_auc: 0.8063, val_auc: 0.7486
Epoch 7, train_loss: 0.4348, val_loss: 0.4938, train_auc: 0.8133, val_auc: 0.7445
Epoch 8, train_loss: 0.4173, val_loss: 0.5021, train_auc: 0.8330, val_auc: 0.7436
Epoch 9, train_loss: 0.3951, val_loss: 0.5216, train_auc: 0.8524, val_auc: 0.7386

# add numerical feature StandardScaler

data shape (1000000, 40)
-------- start training --------
Epoch 0, train_loss: 0.5034, val_loss: 0.4833, train_auc: 0.7289, val_auc: 0.7534
Epoch 1, train_loss: 0.4801, val_loss: 0.4764, train_auc: 0.7636, val_auc: 0.7632
Epoch 2, train_loss: 0.4649, val_loss: 0.4753, train_auc: 0.7833, val_auc: 0.7660
Epoch 3, train_loss: 0.4499, val_loss: 0.4769, train_auc: 0.8012, val_auc: 0.7647
Epoch 4, train_loss: 0.4341, val_loss: 0.4880, train_auc: 0.8184, val_auc: 0.7585
Epoch 5, train_loss: 0.4166, val_loss: 0.4958, train_auc: 0.8356, val_auc: 0.7532
Epoch 6, train_loss: 0.3993, val_loss: 0.5053, train_auc: 0.8509, val_auc: 0.7490
Epoch 7, train_loss: 0.3821, val_loss: 0.5234, train_auc: 0.8650, val_auc: 0.7419
Epoch 8, train_loss: 0.3652, val_loss: 0.5488, train_auc: 0.8777, val_auc: 0.7352
Epoch 9, train_loss: 0.3500, val_loss: 0.5629, train_auc: 0.8880, val_auc: 0.7302


data shape (1000000, 40)
-------- start training --------
 -------- early stop, weight_decay --------
Epoch 0, train_loss: 0.5029, val_loss: 0.4843, train_auc: 0.7297, val_auc: 0.7529
Epoch 1, train_loss: 0.4804, val_loss: 0.4776, train_auc: 0.7633, val_auc: 0.7617
Epoch 2, train_loss: 0.4656, val_loss: 0.4751, train_auc: 0.7825, val_auc: 0.7654
Epoch 3, train_loss: 0.4507, val_loss: 0.4756, train_auc: 0.8005, val_auc: 0.7667
Epoch 4, train_loss: 0.4347, val_loss: 0.4863, train_auc: 0.8180, val_auc: 0.7615
Epoch 5, train_loss: 0.4173, val_loss: 0.4922, train_auc: 0.8350, val_auc: 0.7568
Early stopping

# just early stop parameters
data shape (1000000, 40)
-------- start training --------
 -------- early stop, weight_decay, ReduceLROnPlateau --------
Epoch 0, train_loss: 0.5061, val_loss: 0.4846, train_auc: 0.7248, val_auc: 0.7525, lr: 0.001000
Epoch 1, train_loss: 0.4812, val_loss: 0.4786, train_auc: 0.7623, val_auc: 0.7630, lr: 0.001000
Epoch 2, train_loss: 0.4666, val_loss: 0.4756, train_auc: 0.7816, val_auc: 0.7655, lr: 0.001000
Epoch 3, train_loss: 0.4516, val_loss: 0.4771, train_auc: 0.7994, val_auc: 0.7652, lr: 0.001000
Epoch 4, train_loss: 0.4356, val_loss: 0.4833, train_auc: 0.8169, val_auc: 0.7616, lr: 0.001000
Early stopping



data shape (2000000, 40)
-------- start training --------
 -------- early stop, weight_decay, ReduceLROnPlateau, WideAndDeep --------
Epoch 0, train_loss: 187.5194, val_loss: 10.9658, train_auc: 0.5147, val_auc: 0.5468, lr: 0.001000
Epoch 1, train_loss: 12.2123, val_loss: 5.9619, train_auc: 0.5477, val_auc: 0.6153, lr: 0.001000
Epoch 2, train_loss: 10.8169, val_loss: 3.8596, train_auc: 0.5626, val_auc: 0.6152, lr: 0.001000
Epoch 3, train_loss: 11.6008, val_loss: 9.7381, train_auc: 0.5732, val_auc: 0.5831, lr: 0.001000
Epoch 4, train_loss: 9.0737, val_loss: 4.7941, train_auc: 0.5943, val_auc: 0.6178, lr: 0.001000
Epoch 5, train_loss: 7.4048, val_loss: 2.6746, train_auc: 0.6163, val_auc: 0.6420, lr: 0.001000
Epoch 6, train_loss: 7.1402, val_loss: 3.0975, train_auc: 0.6345, val_auc: 0.6699, lr: 0.001000
Early stopping


data shape (1000000, 40)
-------- start training --------
 -------- early stop, weight_decay, ReduceLROnPlateau, WideAndDeep --------
Epoch 0, train_loss: 145.3186, val_loss: 3.4569, train_auc: 0.5380, val_auc: 0.6238, lr: 0.001000
Epoch 1, train_loss: 6.6126, val_loss: 2.2474, train_auc: 0.5736, val_auc: 0.6386, lr: 0.001000
Epoch 2, train_loss: 6.7364, val_loss: 1.0829, train_auc: 0.5817, val_auc: 0.6765, lr: 0.001000
Epoch 3, train_loss: 5.8264, val_loss: 5.4479, train_auc: 0.5962, val_auc: 0.6791, lr: 0.001000
Epoch 4, train_loss: 4.1331, val_loss: 1.9271, train_auc: 0.6244, val_auc: 0.6379, lr: 0.001000
Epoch 5, train_loss: 3.8958, val_loss: 3.2335, train_auc: 0.6375, val_auc: 0.5974, lr: 0.001000
Epoch 6, train_loss: 3.3419, val_loss: 1.4551, train_auc: 0.6581, val_auc: 0.6887, lr: 0.001000
Epoch 7, train_loss: 2.9463, val_loss: 2.0350, train_auc: 0.6790, val_auc: 0.6803, lr: 0.001000
Early stopping


data shape (1000000, 40)
-------- start training --------
 -------- early stop, weight_decay, ReduceLROnPlateau, WideAndDeep, patience from 3 to 8 --------
Epoch 0, train_loss: 246.2501, val_loss: 23.1281, train_auc: 0.5497, val_auc: 0.6129, lr: 0.001000
Epoch 1, train_loss: 10.2215, val_loss: 4.3550, train_auc: 0.5564, val_auc: 0.5921, lr: 0.001000
Epoch 2, train_loss: 8.4166, val_loss: 3.4582, train_auc: 0.5654, val_auc: 0.6572, lr: 0.001000
Epoch 3, train_loss: 6.0966, val_loss: 1.8095, train_auc: 0.5794, val_auc: 0.6283, lr: 0.001000
Epoch 4, train_loss: 5.9943, val_loss: 1.8040, train_auc: 0.5900, val_auc: 0.6304, lr: 0.001000
Epoch 5, train_loss: 5.1081, val_loss: 3.0591, train_auc: 0.6059, val_auc: 0.6307, lr: 0.001000
Epoch 6, train_loss: 4.8441, val_loss: 3.0270, train_auc: 0.6212, val_auc: 0.6846, lr: 0.001000
Epoch 7, train_loss: 4.5069, val_loss: 1.4676, train_auc: 0.6388, val_auc: 0.6570, lr: 0.001000
Epoch 8, train_loss: 2.9960, val_loss: 1.3381, train_auc: 0.6624, val_auc: 0.6777, lr: 0.001000
Epoch 9, train_loss: 3.2305, val_loss: 2.3445, train_auc: 0.6717, val_auc: 0.6368, lr: 0.001000
Epoch 10, train_loss: 2.8370, val_loss: 2.0042, train_auc: 0.6896, val_auc: 0.6777, lr: 0.001000
Epoch 11, train_loss: 2.5556, val_loss: 3.3072, train_auc: 0.7045, val_auc: 0.6766, lr: 0.001000
Epoch 12, train_loss: 2.3293, val_loss: 2.8045, train_auc: 0.7218, val_auc: 0.6597, lr: 0.001000
Epoch 13, train_loss: 2.7142, val_loss: 3.8112, train_auc: 0.7265, val_auc: 0.6507, lr: 0.001000
Epoch 14, train_loss: 2.3588, val_loss: 4.7761, train_auc: 0.7416, val_auc: 0.6510, lr: 0.001000
Early stopping


 -------- early stop, weight_decay, ReduceLROnPlateau, WideAndDeepV4, one hot encoding (10k training data) --------
 -------- early stop, weight_decay, ReduceLROnPlateau, WideAndDeepV4 --------
Epoch 0, train_loss: 0.6082, val_loss: 0.5247, train_auc: 0.5257, val_auc: 0.5974, lr: 0.001000
Epoch 1, train_loss: 0.5152, val_loss: 0.5099, train_auc: 0.6207, val_auc: 0.6779, lr: 0.001000
Epoch 2, train_loss: 0.4826, val_loss: 0.5038, train_auc: 0.7333, val_auc: 0.7139, lr: 0.001000
Epoch 3, train_loss: 0.4614, val_loss: 0.4923, train_auc: 0.7746, val_auc: 0.7140, lr: 0.001000
Epoch 4, train_loss: 0.4472, val_loss: 0.4867, train_auc: 0.7848, val_auc: 0.7178, lr: 0.001000
Epoch 5, train_loss: 0.4306, val_loss: 0.4841, train_auc: 0.8151, val_auc: 0.7198, lr: 0.001000
Epoch 6, train_loss: 0.4181, val_loss: 0.4839, train_auc: 0.8251, val_auc: 0.7215, lr: 0.001000
Epoch 7, train_loss: 0.4031, val_loss: 0.4830, train_auc: 0.8416, val_auc: 0.7226, lr: 0.001000
Epoch 8, train_loss: 0.3875, val_loss: 0.4846, train_auc: 0.8603, val_auc: 0.7229, lr: 0.001000
Epoch 9, train_loss: 0.3717, val_loss: 0.4880, train_auc: 0.8752, val_auc: 0.7203, lr: 0.001000
Early stopping



data shape (50000, 40)
train_df_ohe shape  (40000, 119609)
-------- start training --------
 -------- early stop, weight_decay, ReduceLROnPlateau, WideAndDeepV4 (50k training data) --------
Epoch 0, train_loss: 0.5225, val_loss: 0.4878, train_auc: 0.6143, val_auc: 0.7197, lr: 0.001000
Epoch 1, train_loss: 0.4660, val_loss: 0.4738, train_auc: 0.7529, val_auc: 0.7310, lr: 0.001000
Epoch 2, train_loss: 0.4408, val_loss: 0.4700, train_auc: 0.7896, val_auc: 0.7375, lr: 0.001000
Epoch 3, train_loss: 0.4211, val_loss: 0.4688, train_auc: 0.8170, val_auc: 0.7422, lr: 0.001000
Epoch 4, train_loss: 0.3980, val_loss: 0.4733, train_auc: 0.8392, val_auc: 0.7377, lr: 0.001000


data shape (1000000, 40)
-------- start training --------
 -------- DeepFM --------
Epoch 0, train_loss: 31.5386, val_loss: 12.8867, train_auc: 0.5970, val_auc: 0.6374, lr: 0.001000
Epoch 1, train_loss: 12.8392, val_loss: 10.4614, train_auc: 0.6366, val_auc: 0.6420, lr: 0.001000
Epoch 2, train_loss: 10.5593, val_loss: 9.7349, train_auc: 0.6525, val_auc: 0.6402, lr: 0.001000
Epoch 3, train_loss: 9.2115, val_loss: 9.4656, train_auc: 0.6668, val_auc: 0.6455, lr: 0.001000
Epoch 4, train_loss: 8.0823, val_loss: 9.7521, train_auc: 0.6809, val_auc: 0.6475, lr: 0.001000
Epoch 5, train_loss: 7.1751, val_loss: 9.1231, train_auc: 0.6940, val_auc: 0.6411, lr: 0.001000
Epoch 6, train_loss: 6.6844, val_loss: 9.6860, train_auc: 0.7078, val_auc: 0.6428, lr: 0.001000
Early stopping



data shape (1000000, 40), emb from 12 to 32, add weight_decay for L2 regularization, change hidden layer size
-------- start training --------
 -------- DeepFM --------
Epoch 0, train_loss: 105.3346, val_loss: 31.5538, train_auc: 0.5909, val_auc: 0.6423, lr: 0.001000
Epoch 1, train_loss: 33.3792, val_loss: 23.7683, train_auc: 0.6360, val_auc: 0.6575, lr: 0.001000
Epoch 2, train_loss: 24.1684, val_loss: 20.3555, train_auc: 0.6500, val_auc: 0.6576, lr: 0.001000
Epoch 3, train_loss: 20.8479, val_loss: 17.7893, train_auc: 0.6559, val_auc: 0.6591, lr: 0.001000

data shape (1000000, 40)
-------- start training --------
 -------- DeepFM , dropout 0.3--------
Epoch 0, train_loss: 103.4737, val_loss: 32.1828, train_auc: 0.5935, val_auc: 0.6408, lr: 0.001000
Epoch 1, train_loss: 36.8029, val_loss: 23.8940, train_auc: 0.6386, val_auc: 0.6460, lr: 0.001000
Epoch 2, train_loss: 27.1621, val_loss: 21.8542, train_auc: 0.6480, val_auc: 0.6626, lr: 0.001000
Epoch 3, train_loss: 23.3605, val_loss: 18.9304, train_auc: 0.6528, val_auc: 0.6510, lr: 0.001000
Epoch 4, train_loss: 20.4780, val_loss: 18.2109, train_auc: 0.6568, val_auc: 0.6538, lr: 0.001000
Epoch 5, train_loss: 18.0456, val_loss: 17.5643, train_auc: 0.6615, val_auc: 0.6488, lr: 0.001000
Epoch 6, train_loss: 16.7281, val_loss: 16.6549, train_auc: 0.6640, val_auc: 0.6497, lr: 0.001000
Epoch 7, train_loss: 15.3092, val_loss: 16.6400, train_auc: 0.6676, val_auc: 0.6488, lr: 0.001000
Epoch 8, train_loss: 14.2625, val_loss: 16.7372, train_auc: 0.6714, val_auc: 0.6521, lr: 0.001000
Epoch 9, train_loss: 13.2841, val_loss: 16.8253, train_auc: 0.6733, val_auc: 0.6479, lr: 0.001000
Early stopping


data shape (1000000, 40)
-------- start training --------
 -------- DeepFM, Numeric embeddings: Replaces repeated scalars → richer interactions in FM (emb_dim=32, hidden_dims=[128, 64, 32]) --------
Epoch 0, train_loss: 24.5372, val_loss: 14.3797, train_auc: 0.6089, val_auc: 0.6429, lr: 0.001000
Epoch 1, train_loss: 10.9941, val_loss: 8.3212, train_auc: 0.6563, val_auc: 0.6585, lr: 0.001000
Epoch 2, train_loss: 6.4914, val_loss: 5.3394, train_auc: 0.6753, val_auc: 0.6665, lr: 0.001000
Epoch 3, train_loss: 4.1441, val_loss: 3.5520, train_auc: 0.6888, val_auc: 0.6716, lr: 0.001000
Epoch 4, train_loss: 2.7588, val_loss: 2.5101, train_auc: 0.6988, val_auc: 0.6757, lr: 0.001000
Epoch 5, train_loss: 1.9010, val_loss: 1.7478, train_auc: 0.7076, val_auc: 0.6858, lr: 0.001000
Epoch 6, train_loss: 1.3627, val_loss: 1.2862, train_auc: 0.7156, val_auc: 0.6879, lr: 0.001000
Epoch 7, train_loss: 1.0264, val_loss: 0.9980, train_auc: 0.7231, val_auc: 0.6903, lr: 0.001000
Epoch 8, train_loss: 0.8101, val_loss: 0.8247, train_auc: 0.7310, val_auc: 0.7014, lr: 0.001000
Epoch 9, train_loss: 0.6875, val_loss: 0.7202, train_auc: 0.7384, val_auc: 0.7134, lr: 0.001000
Epoch 10, train_loss: 0.6234, val_loss: 0.6747, train_auc: 0.7453, val_auc: 0.7135, lr: 0.001000
Epoch 11, train_loss: 0.5727, val_loss: 0.6175, train_auc: 0.7524, val_auc: 0.7251, lr: 0.001000
Epoch 12, train_loss: 0.5460, val_loss: 0.5863, train_auc: 0.7574, val_auc: 0.7290, lr: 0.001000
Epoch 13, train_loss: 0.5399, val_loss: 0.5837, train_auc: 0.7604, val_auc: 0.7255, lr: 0.001000
Epoch 14, train_loss: 0.5210, val_loss: 0.5613, train_auc: 0.7646, val_auc: 0.7340, lr: 0.001000
Epoch 15, train_loss: 0.5122, val_loss: 0.5527, train_auc: 0.7668, val_auc: 0.7338, lr: 0.001000
Epoch 16, train_loss: 0.5067, val_loss: 0.5465, train_auc: 0.7694, val_auc: 0.7435, lr: 0.001000
Epoch 17, train_loss: 0.5049, val_loss: 0.5597, train_auc: 0.7712, val_auc: 0.7410, lr: 0.001000
Epoch 18, train_loss: 0.4966, val_loss: 0.5336, train_auc: 0.7735, val_auc: 0.7444, lr: 0.001000
Epoch 19, train_loss: 0.4951, val_loss: 0.6402, train_auc: 0.7742, val_auc: 0.7458, lr: 0.001000


-------- start training --------
 -------- DCN --------
 ==========================================================================================
Total params: 35,047,068
Trainable params: 35,047,068
Non-trainable params: 0
Total mult-adds (G): 35.89
==========================================================================================
Input size (MB): 0.16
Forward/backward pass size (MB): 29.43
Params size (MB): 140.19
Estimated Total Size (MB): 169.77

Epoch 0, train_loss: 0.5098, val_loss: 0.4926, train_auc: 0.7207, val_auc: 0.7403, lr: 0.001000
Epoch 1, train_loss: 0.4951, val_loss: 0.4862, train_auc: 0.7428, val_auc: 0.7491, lr: 0.001000
Epoch 2, train_loss: 0.4889, val_loss: 0.4817, train_auc: 0.7514, val_auc: 0.7571, lr: 0.001000
Epoch 3, train_loss: 0.4900, val_loss: 0.4784, train_auc: 0.7594, val_auc: 0.7643, lr: 0.001000
Epoch 4, train_loss: 0.4774, val_loss: 0.4710, train_auc: 0.7677, val_auc: 0.7706, lr: 0.001000
Epoch 5, train_loss: 0.4737, val_loss: 0.4713, train_auc: 0.7723, val_auc: 0.7738, lr: 0.001000
Epoch 6, train_loss: 0.4881, val_loss: 0.4668, train_auc: 0.7760, val_auc: 0.7760, lr: 0.001000
Epoch 7, train_loss: 0.4683, val_loss: 0.4675, train_auc: 0.7799, val_auc: 0.7784, lr: 0.001000
Early stopping





Total params: 35,153,538
Trainable params: 35,153,538
Non-trainable params: 0
Total mult-adds (G): 35.99
==========================================================================================
Input size (MB): 0.16
Forward/backward pass size (MB): 36.87
Params size (MB): 140.60
Estimated Total Size (MB): 177.63
==========================================================================================
-------- start training --------
 -------- DCN V2 --------
Epoch 0, train_loss: 0.5034, val_loss: 0.4854, train_auc: 0.7349, val_auc: 0.7520, lr: 0.000800
Epoch 1, train_loss: 0.4991, val_loss: 0.4842, train_auc: 0.7540, val_auc: 0.7541, lr: 0.000800
Epoch 2, train_loss: 0.4859, val_loss: 0.4839, train_auc: 0.7560, val_auc: 0.7555, lr: 0.000800
Epoch 3, train_loss: 0.6047, val_loss: 0.5433, train_auc: 0.7578, val_auc: 0.7592, lr: 0.000800
Epoch 4, train_loss: 0.9769, val_loss: 0.4751, train_auc: 0.7635, val_auc: 0.7660, lr: 0.000800
Epoch 5, train_loss: 0.4765, val_loss: 0.4711, train_auc: 0.7711, val_auc: 0.7710, lr: 0.000800
Epoch 6, train_loss: 0.4694, val_loss: 0.4686, train_auc: 0.7779, val_auc: 0.7761, lr: 0.000800
Epoch 7, train_loss: 0.4645, val_loss: 0.4699, train_auc: 0.7841, val_auc: 0.7773, lr: 0.000800
Epoch 8, train_loss: 0.4603, val_loss: 0.4654, train_auc: 0.7893, val_auc: 0.7790, lr: 0.000800
Epoch 9, train_loss: 0.4567, val_loss: 0.4652, train_auc: 0.7935, val_auc: 0.7794, lr: 0.000800
Epoch 10, train_loss: 0.4525, val_loss: 0.4660, train_auc: 0.7985, val_auc: 0.7788, lr: 0.000800
Epoch 11, train_loss: 0.4452, val_loss: 0.4688, train_auc: 0.8070, val_auc: 0.7768, lr: 0.000800
Early stopping
 lr:  0.000800 weight_decay:  0.000500 rank:  16

"""
